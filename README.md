# Self-Hosted Llama AI Agent

> A project to run **Llama 3.1** locally using **Ollama** and **n8n**, creating a self-hosted AI automation workflow.

---

## Table of Contents

- [Project Overview](#project-overview)
- [Features](#features)
- [Key Steps &amp; Components](#key-steps--components)
- [Outcomes](#outcomes)
- [Tech Stack](#tech-stack)
- [Setup &amp; Usage](#setup--usage)
- [References](#references)

---

## Project Overview

This project demonstrates how to run **Llama 3.1**, Meta’s advanced open-source large language model, **locally** on your computer using **Ollama** and **n8n**. The goal was to build a **self-hosted AI automation workflow** without relying on cloud services, ensuring **cost efficiency**, **data privacy**, and **complete control** over AI workflows.

With this setup, users can automate AI tasks, process prompts, and generate responses **entirely locally**, making it ideal for developers, AI enthusiasts, and businesses seeking private AI solutions.

---

## Features

- Run **Llama 3.1** and **Gemma 2** models locally without cloud dependencies.
- Integrate AI models with **n8n workflow automation**.
- Create custom **AI agents** for task automation.
- Optimize AI performance for local deployment.
- Containerized setup using **Docker** for reproducibility.

---

## Key Steps & Components

1. **Installed Ollama**

   - Set up Ollama to manage and run local AI models efficiently.
2. **Downloaded Models**

   - Installed **Llama 3.1** and **Gemma 2** for local inference.
3. **Configured n8n**

   - Deployed the **n8n Self-Hosted AI Starter Kit** to automate workflows and integrate local AI models.
4. **Integrated Ollama with n8n**

   - Created a custom **AI Agent** node using the Ollama Chat Model.
5. **Workflow Automation**

   - Designed workflows to process prompts, generate responses, and automate tasks with n8n.

---

## Outcomes

- Successfully ran **Llama 3.1 locally** using Ollama.
- Achieved **end-to-end AI automation** with n8n’s workflow engine.
- Gained hands-on experience with **self-hosted AI infrastructure**, **model deployment**, and **workflow integration**.
- Learned best practices for **optimizing local AI performance** and managing open-source LLMs.

---

## Tech Stack

- **Ollama** – Local LLM hosting
- **n8n** – Workflow automation engine
- **Docker & Docker Desktop** – Containerized deployment
- **Llama 3.1 & Gemma 2** – AI models

---

## Setup & Usage

1. **Install Ollama**: [Ollama Installation Guide](https://ollama.com)
2. **Download AI Models**: Llama 3.1 and Gemma 2
3. **Install n8n**: [n8n Setup](https://n8n.io)
4. **Configure n8n AI Agent**: Connect Ollama Chat Model to n8n workflows.
5. **Run Workflows**: Automate tasks using local AI models via n8n.

---

## References

- [Ollama](https://ollama.com)
- [n8n Self-Hosted AI Starter Kit](https://github.com/n8n-io/self-hosted-ai-starter-kit)
- [Meta AI Llama 3.1](https://ai.meta.com/llama/)

---

**Author:** Rohan Rajkumar Tekale
**Date:** September 2024
